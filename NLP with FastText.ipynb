{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wikipedia\n",
      "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\jawha\\anaconda3\\lib\\site-packages (from wikipedia) (4.8.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in c:\\users\\jawha\\anaconda3\\lib\\site-packages (from wikipedia) (2.22.0)\n",
      "Requirement already satisfied: soupsieve>=1.2 in c:\\users\\jawha\\anaconda3\\lib\\site-packages (from beautifulsoup4->wikipedia) (1.9.5)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\jawha\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.25.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\jawha\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jawha\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2019.11.28)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\jawha\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.8)\n",
      "Building wheels for collected packages: wikipedia\n",
      "  Building wheel for wikipedia (setup.py): started\n",
      "  Building wheel for wikipedia (setup.py): finished with status 'done'\n",
      "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11689 sha256=0f0154332a574429cd6161343bb4806d41a4bada1814fb554f58aa761adc6f22\n",
      "  Stored in directory: c:\\users\\jawha\\appdata\\local\\pip\\cache\\wheels\\15\\93\\6d\\5b2c68b8a64c7a7a04947b4ed6d89fb557dcc6bc27d1d7f3ba\n",
      "Successfully built wikipedia\n",
      "Installing collected packages: wikipedia\n",
      "Successfully installed wikipedia-1.4.0\n"
     ]
    }
   ],
   "source": [
    " !pip install wikipedia "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jawha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\jawha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jawha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer #separation split des text\n",
    "from gensim.models.fasttext import FastText \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import nltk \n",
    "from string import punctuation \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.tokenize import sent_tokenize \n",
    "from nltk import WordPunctTokenizer\n",
    "import wikipedia \n",
    "import nltk \n",
    "\n",
    "nltk.download('punkt') \n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "en_stop = set(nltk.corpus.stopwords.words('english'))\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "artificial_intelligence = wikipedia.page(\"Artificial Intelligence\").content\n",
    "machine_learning = wikipedia.page(\"Machine Learning\").content\n",
    "deep_learning = wikipedia.page(\"Deep Learning\").content \n",
    "neural_network = wikipedia.page(\"Neural Network\").content \n",
    "artificial_intelligence = sent_tokenize(artificial_intelligence) \n",
    "machine_learning = sent_tokenize(machine_learning) \n",
    "deep_learning = sent_tokenize(deep_learning) \n",
    "neural_network = sent_tokenize(neural_network)\n",
    "artificial_intelligence.extend(machine_learning) \n",
    "artificial_intelligence.extend(deep_learning) \n",
    "artificial_intelligence.extend(neural_network)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "stemmer = WordNetLemmatizer()\n",
    "def preprocess_text(document):\n",
    "    # Remove all the special characters\n",
    "    document = re.sub(r'\\W', ' ', str(document))\n",
    "    # remove all single characters    \n",
    "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)   \n",
    "    # Remove single characters from the start     \n",
    "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document)  \n",
    "    # Substituting multiple spaces with single space      \n",
    "    document = re.sub(r'\\s+', ' ', document, flags=re.I)   \n",
    "    # Removing prefixed 'b'    \n",
    "    document = re.sub(r'^b\\s+', '', document)\n",
    "    document = document.lower()   \n",
    "    # Lemmatization    \n",
    "    tokens = document.split() \n",
    "    tokens = [stemmer.lemmatize(word) for word in tokens]   \n",
    "    tokens = [word for word in tokens if word not in en_stop]\n",
    "    tokens = [word for word in tokens if len(word) > 3]  \n",
    "    preprocessed_text = ' '.join(tokens)    \n",
    "    return preprocessed_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "artificial intelligence advanced technology present\n"
     ]
    }
   ],
   "source": [
    "sent = preprocess_text(\"Artificial intelligence, is the most advanced technology of the present era\")\n",
    "print(sent) \n",
    "final_corpus = [preprocess_text(sentence) for sentence in artificial_intelligence if sentence.strip() !='']\n",
    "word_punctuation_tokenizer = nltk.WordPunctTokenizer()\n",
    "word_tokenized_corpus = [word_punctuation_tokenizer.tokenize(sent) for sent in final_corpus]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 60\n",
    "window_size = 40 \n",
    "min_word = 5 \n",
    "down_sampling = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 36 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "ft_model = FastText(word_tokenized_corpus,  \n",
    "                    size=embedding_size,     \n",
    "                    window=window_size,        \n",
    "                    min_count=min_word,      \n",
    "                    sample=down_sampling,      \n",
    "                    sg=1,                     \n",
    "                    iter=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.11839376 -0.03812832 -0.11664832  0.11711448 -0.12049824  0.06692523\n",
      " -0.12682211  0.09924031 -0.30844578  0.12188611  0.15434428  0.1214325\n",
      "  0.14461546 -0.0095361  -0.02415161  0.41041502 -0.09409902  0.42355317\n",
      " -0.0181209  -0.10814302  0.08013644 -0.18074793  0.1334448   0.23130922\n",
      "  0.04619735  0.20290028 -0.10390455 -0.22687817 -0.01655591 -0.05352966\n",
      " -0.19269606  0.15170705  0.24634314  0.05126771  0.15523972  0.34458855\n",
      " -0.06066853  0.11028368 -0.07255322  0.11476649 -0.19649726  0.18181421\n",
      "  0.01918208 -0.17781496  0.01979229 -0.31729764  0.02896908  0.02266075\n",
      " -0.00577541 -0.17185129 -0.18559833  0.06065497 -0.00189912  0.07369715\n",
      "  0.07284637 -0.09339489  0.02347735  0.1270955   0.08413529  0.34950942]\n"
     ]
    }
   ],
   "source": [
    " print(ft_model.wv['artificial'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
